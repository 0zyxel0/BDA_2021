{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries to be used\n",
    "- !pip install -U pip setuptools wheel\n",
    "- !pip install -U spacy\n",
    "- !python -m spacy download en_core_web_sm\n",
    "- !pip install emot --upgrade\n",
    "- !pip install emoji --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "import random \n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "import emoji\n",
    "import spacy\n",
    "from os import walk\n",
    "import timeit\n",
    "\n",
    "\n",
    "# Load the fields list\n",
    "import fields\n",
    "fieldsFilter = fields.fields\n",
    "\n",
    "# Declare Spacy NLP Module\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat a small sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_samples = \"200\"\n",
    "list_tweets = None\n",
    "\n",
    "with open(\"./tweets/20200916.json\", \"r\") as myfile:\n",
    "    list_tweets = list(myfile)\n",
    "\n",
    "if int(no_samples) > len(list_tweets):\n",
    "    no_samples = len(list_tweets)\n",
    "\n",
    "sample = random.sample(list_tweets, int(no_samples))\n",
    "\n",
    "file = open(\"sample_data.json\", \"w\")\n",
    "for i in sample:\n",
    "  file.write(i)\n",
    "file.close() #This close() is important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_top_unique_words = \"50\" #@param {type:\"string\"}\n",
    "\n",
    "result = Counter(\" \".join(tweet_df['text'].values.tolist()).split(\" \")).items()\n",
    "df2 = pd.DataFrame(result)\n",
    "df2.columns =['Word', 'Frequency']\n",
    "df2 = df2[df2.Word != \"\"] #Deletes the empty spaces counted\n",
    "df2 = df2.sort_values(['Frequency'], ascending=[False]) #Sort dataframe by frequency (Descending)\n",
    "\n",
    "print('\\033[1mTop '+no_top_unique_words+' most unique words used from the dataset\\033[0m \\n')\n",
    "print(df2.head(int(no_top_unique_words)).to_string(index=False)) #Prints the top N unique words used\n",
    "print(\"\\n\")\n",
    "df3 = df2.head(int(no_top_unique_words))\n",
    "df3.plot(y='Frequency', kind='pie', labels=df3['Word'], figsize=(9, 9), autopct='%1.1f%%', title='Top '+no_top_unique_words+' most unique words used from the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_tweets = []\n",
    "with open('./tweets/sample_data.json', 'r') as f:\n",
    "    for line in f:\n",
    "        complete_tweets.append(json.loads(line))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datfile = 's1_sa_dsaf'\n",
    "fname = str(datfile).split('_')[1]\n",
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEANING  FUNCTIONS\n",
    "\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    result = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return(result)\n",
    "\n",
    "def remove_twitter_urls(text):\n",
    "    clean = re.sub(r\"pic.twitter\\S+\", \"\",text)\n",
    "    return(clean)\n",
    "\n",
    "def give_emoji_free_text(text):\n",
    "    return emoji.get_emoji_regexp().sub(r'', text)\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    clean_tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
    "    clean_tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", clean_tweet)\n",
    "    return clean_tweet\n",
    "\n",
    "def change_to_lowercase(text):\n",
    "    return text.lower()    \n",
    "\n",
    "# def remove_connecting_words(text):    \n",
    "#     clean_tweet = re.sub('\\s+(a|an|and|the|is|from|as|our|it|in|the|i|by|at|to|of|or|he|on|be|this|up|so|are|has|if|was|for|we)(\\s+)', '\\2', text)\n",
    "#     return clean_tweet\n",
    "\n",
    "def remove_returnchar(text):\n",
    "    clean_tweet = text.replace('\\r','')\n",
    "    return clean_tweet\n",
    "\n",
    "def remove_newlines(text):\n",
    "    clean_tweet = text.replace('\\n','')\n",
    "    return clean_tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTweetText(datfile):\n",
    "    \n",
    "    #Initialize Temporary Array Storage\n",
    "    tempStorage = []\n",
    "\n",
    "    #Parse Json to a temporary storage\n",
    "    with open('./tweets/'+datfile, 'r') as f:\n",
    "        for line in f:\n",
    "            tempStorage.append(json.loads(line))\n",
    "\n",
    "    # Convert Array to a Pandas Data Frame\n",
    "    tempFrame = pd.DataFrame(tempStorage)\n",
    "    \n",
    "    # Drop all columns except for text column\n",
    "    tempFrame = tempFrame[['text']]\n",
    "    \n",
    "    #Get only the file name and remove the extension\n",
    "    fname = str(datfile).split('.')[0]\n",
    "    \n",
    "    #Save dataframe to csv\n",
    "    tempFrame.to_csv(r'./step_1/s1_'+str(fname)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and process tweet text by removing URL, EMOJI and User Mentions\n",
    "def removeTwitterMeta(datfile):    \n",
    "    tweet_df = pd.read_csv('./step_1/'+str(datfile),index_col=False) \n",
    "    #Replace the spaces and enters\n",
    "    tweet_df = tweet_df.loc[:, tweet_df.columns.isin(fieldsFilter)]\n",
    "    tweet_df['text'] = tweet_df['text'].apply(lambda x : remove_newlines(x))\n",
    "    tweet_df['text'] = tweet_df['text'].apply(lambda x : remove_returnchar(x))\n",
    "    tweet_df['text'] = tweet_df['text'].apply(lambda x : remove_urls(x))\n",
    "    tweet_df['text'] = tweet_df['text'].apply(lambda x : remove_twitter_urls(x))\n",
    "    tweet_df['text'] = tweet_df['text'].apply(lambda x : remove_emoticons(x))\n",
    "    tweet_df['text'] = tweet_df['text'].apply(lambda x : remove_emoji(x))\n",
    "    tweet_df['text'] = tweet_df['text'].apply(lambda x : give_emoji_free_text(x))\n",
    "    tweet_df['text'] = tweet_df['text'].apply(lambda x : remove_user_mentions(x))\n",
    "    tweet_df['text'] = tweet_df['text'].apply(lambda x : change_to_lowercase(x))\n",
    "\n",
    "    #Get only the file name and remove the extension\n",
    "    fname = str(datfile).split('_')[1]\n",
    "    fname = str(fname).split('.')[0]\n",
    "    tweet_df.to_csv(r'./step_2/s2_'+str(fname)+'.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text in the dataframe\n",
    "\n",
    "def tokenizeText(data):\n",
    "    tempStorage = []\n",
    "    doc = nlp(data)\n",
    "    for token in doc:\n",
    "        tempStorage.append({token.text,token.pos_})\n",
    "    return tempStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0\n",
    "# Specify Raw Tweet Directory\n",
    "tweetDir = './tweets'\n",
    "_, _, filenames = next(walk(tweetDir))\n",
    "\n",
    "\n",
    "for file in filenames:\n",
    "    extractTweetText(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "#Extract Text of Tweets and Save to CSV file\n",
    "\n",
    "tweetDirS1 = './step_1'\n",
    "_, _, filenames_s1 = next(walk(tweetDirS1))\n",
    "\n",
    "for file in filenames_s1:\n",
    "    removeTwitterMeta(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s2_sample.csv']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2\n",
    "# Get The Files inside the step_2 folder for futher processing\n",
    "tweetDirS2 = './step_2'\n",
    "_, _, filenames_s2 = next(walk(tweetDirS2))\n",
    "filenames_s2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "mydata = pd.read_csv('./step_2/s2_sample.csv',index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata\n",
    "\n",
    "        \n",
    "tokenizeText(mydata.loc[0].text)\n",
    "mydata['ttext'] = mydata['text'].apply(lambda x : tokenizeText(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(mydata.loc[0].text)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>ttext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>so all you that say trump never downplayed cov...</td>\n",
       "      <td>[{ADV, so}, {all, DET}, {PRON, you}, {that, DE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>coronavirus: yorkshire quartet out of t20 blas...</td>\n",
       "      <td>[{NOUN, coronavirus}, {PUNCT, :}, {NOUN, yorks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>cdc: almost all of the us kids and teens who'v...</td>\n",
       "      <td>[{NOUN, cdc}, {PUNCT, :}, {ADV, almost}, {all,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>fellow south africans must one day tell ukuthi...</td>\n",
       "      <td>[{ADJ, fellow}, {ADJ, south}, {africans, NOUN}...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>ahmedabad adds 165 covid-19 cases, taking tall...</td>\n",
       "      <td>[{NOUN, ahmedabad}, {adds, VERB}, {165, NUM}, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>but the riverton fancy car gathering and the c...</td>\n",
       "      <td>[{but, CCONJ}, {the, DET}, {ADJ, riverton}, {A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>the majority of children who die from covid-19...</td>\n",
       "      <td>[{the, DET}, {NOUN, majority}, {of, ADP}, {chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197</td>\n",
       "      <td>oh god.</td>\n",
       "      <td>[{INTJ, oh}, {god, INTJ}, {PUNCT, .}]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>one of the challenges the us faces during the...</td>\n",
       "      <td>[{ , SPACE}, {NUM, one}, {of, ADP}, {the, DET}...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>meant to be going away golfing next week for a...</td>\n",
       "      <td>[{meant, VERB}, {PART, to}, {AUX, be}, {going,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                               text  \\\n",
       "0             0  so all you that say trump never downplayed cov...   \n",
       "1             1  coronavirus: yorkshire quartet out of t20 blas...   \n",
       "2             2  cdc: almost all of the us kids and teens who'v...   \n",
       "3             3  fellow south africans must one day tell ukuthi...   \n",
       "4             4  ahmedabad adds 165 covid-19 cases, taking tall...   \n",
       "..          ...                                                ...   \n",
       "195         195  but the riverton fancy car gathering and the c...   \n",
       "196         196  the majority of children who die from covid-19...   \n",
       "197         197                                           oh god.    \n",
       "198         198   one of the challenges the us faces during the...   \n",
       "199         199  meant to be going away golfing next week for a...   \n",
       "\n",
       "                                                 ttext  \n",
       "0    [{ADV, so}, {all, DET}, {PRON, you}, {that, DE...  \n",
       "1    [{NOUN, coronavirus}, {PUNCT, :}, {NOUN, yorks...  \n",
       "2    [{NOUN, cdc}, {PUNCT, :}, {ADV, almost}, {all,...  \n",
       "3    [{ADJ, fellow}, {ADJ, south}, {africans, NOUN}...  \n",
       "4    [{NOUN, ahmedabad}, {adds, VERB}, {165, NUM}, ...  \n",
       "..                                                 ...  \n",
       "195  [{but, CCONJ}, {the, DET}, {ADJ, riverton}, {A...  \n",
       "196  [{the, DET}, {NOUN, majority}, {of, ADP}, {chi...  \n",
       "197              [{INTJ, oh}, {god, INTJ}, {PUNCT, .}]  \n",
       "198  [{ , SPACE}, {NUM, one}, {of, ADP}, {the, DET}...  \n",
       "199  [{meant, VERB}, {PART, to}, {AUX, be}, {going,...  \n",
       "\n",
       "[200 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if\n",
      "beyond\n",
      "with\n",
      "afterwards\n",
      "such\n",
      "along\n",
      "just\n",
      "now\n",
      "back\n",
      "hers\n"
     ]
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "len(spacy_stopwords)\n",
    "\n",
    "for stop_word in list(spacy_stopwords)[:10]:\n",
    "    print(stop_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run for the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run with the sample Dataset using Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
