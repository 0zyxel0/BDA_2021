{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Libraries to be used\n",
    "- !pip install -U pip setuptools wheel\n",
    "- !pip install -U spacy\n",
    "- !python -m spacy download en_core_web_sm\n",
    "- !pip install emot --upgrade\n",
    "- !pip install emoji --upgrade\n",
    "- !pip install spacymoji\n",
    "- !pip install spacytextblob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries that will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Dependencies\n",
    "import ast\n",
    "import json\n",
    "import random \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Load the fields list\n",
    "import fields\n",
    "fieldsFilter = fields.fields\n",
    "\n",
    "# Wordcloud Dependencies\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Emoji Dependencies\n",
    "import unicodedata\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
    "import emoji\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the NLP Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare Spacy NLP Module\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "# Load English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe('spacytextblob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataSample(datafile = \"\",sample_count = 0):\n",
    "    \n",
    "    '''\n",
    "        Creates a sample dataset from a given json file\n",
    "        @param {'type':string} datafile - file path of the twitter json file\n",
    "        @param {'type':string} sample_count - Number of samples to create\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    list_tweets = None\n",
    "    \n",
    "    with open(datafile, \"r\") as myfile:        \n",
    "        list_tweets = list(myfile)\n",
    "\n",
    "    if int(sample_count) > len(list_tweets):\n",
    "        sample_count = len(list_tweets)\n",
    "\n",
    "    sample = random.sample(list_tweets, int(sample_count))\n",
    "    \n",
    "    file = open(\"./tweets/sample_\"+str(sample_count)+\".json\", \"w\")\n",
    "    for i in sample:\n",
    "        file.write(i)\n",
    "    file.close() #This close() is important\n",
    "\n",
    "    return print(\"Sample File Created.\")\n",
    "\n",
    "\n",
    "# CLEANING  FUNCTIONS\n",
    "\n",
    "def remove_emoticons(text):\n",
    "    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n",
    "    return emoticon_pattern.sub(r'', text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_urls(text):\n",
    "    result = re.sub(r\"http\\S+\", \"\", text)\n",
    "    return(result)\n",
    "\n",
    "def remove_twitter_urls(text):\n",
    "    clean = re.sub(r\"pic.twitter\\S+\", \"\",text)\n",
    "    return(clean)\n",
    "\n",
    "def give_emoji_free_text(text):\n",
    "    return emoji.get_emoji_regexp().sub(r'', text)\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    clean_tweet = re.sub(\"@[A-Za-z0-9_]+\",\"\", text)\n",
    "    clean_tweet = re.sub(\"#[A-Za-z0-9_]+\",\"\", clean_tweet)\n",
    "    return clean_tweet\n",
    "\n",
    "def change_to_lowercase(text):\n",
    "    return text.lower()    \n",
    "\n",
    "def remove_returnchar(text):\n",
    "    clean_tweet = text.replace('\\r','')\n",
    "    return clean_tweet\n",
    "\n",
    "def remove_whitespaces(text):\n",
    "    clean_tweet = text.replace(r'(\\r\\n)+|\\r+|\\n+|\\t+','')\n",
    "    return clean_tweet\n",
    "    \n",
    "def extractTweetText(datfile):\n",
    "    \n",
    "    #Initialize Temporary Array Storage\n",
    "    tempStorage = []\n",
    "\n",
    "    #Parse Json to a temporary storage\n",
    "    with open('./tweets/'+datfile, 'r') as f:\n",
    "        for line in f:\n",
    "            tempStorage.append(json.loads(line))\n",
    "\n",
    "    # Convert Array to a Pandas Data Frame\n",
    "    tempFrame = pd.DataFrame(tempStorage)\n",
    "    \n",
    "    # Drop all columns except for text column\n",
    "    tempFrame = tempFrame[['text']]\n",
    "    \n",
    "    #Get only the file name and remove the extension\n",
    "    fname = str(datfile).split('.')[0]\n",
    "    \n",
    "    #Save dataframe to csv\n",
    "    tempFrame.to_csv(r'./step_1/s1_'+str(fname)+'.csv')\n",
    "\n",
    "# Clean and process tweet text by removing URL, EMOJI and User Mentions\n",
    "def removeTwitterMeta(datfile):    \n",
    "    tweet_df = pd.read_csv('./step_1/'+str(datfile),index_col=False) \n",
    "    #Replace the spaces and enters\n",
    "    tweet_df = tweet_df.loc[:, tweet_df.columns.isin(fieldsFilter)]\n",
    "    print(\"Step 1 : Cleaning Whitespaces\")\n",
    "    tweet_df['text'] = tweet_df['text'].progress_apply(lambda x : remove_whitespaces(x))\n",
    "    print(\"Step 2 : Cleaning URLs\")\n",
    "    tweet_df['text'] = tweet_df['text'].progress_apply(lambda x : remove_urls(x))\n",
    "    print(\"Step 3 : Cleaning Twitter URLs\")\n",
    "    tweet_df['text'] = tweet_df['text'].progress_apply(lambda x : remove_twitter_urls(x))\n",
    "    print(\"Step 4 : Converting Emoji To Words\")\n",
    "    tweet_df['text'] = tweet_df['text'].progress_apply(lambda x : emoji_to_word(x))\n",
    "    print(\"Step 5 : Cleaning Emoticons\")\n",
    "    tweet_df['text'] = tweet_df['text'].progress_apply(lambda x : remove_emoticons(x))\n",
    "    print(\"Step 6 : Cleaning Twitter User Mentions\")\n",
    "    tweet_df['text'] = tweet_df['text'].progress_apply(lambda x : remove_user_mentions(x))\n",
    "    print(\"Step 6 : Converting To Lower Case\")\n",
    "    tweet_df['text'] = tweet_df['text'].progress_apply(lambda x : change_to_lowercase(x))\n",
    "\n",
    "    #Get only the file name and remove the extension\n",
    "    fname = str(datfile).split('_')[1]\n",
    "    fname = str(fname).split('.')[0]\n",
    "    tweet_df.to_csv(r'./step_2/s2_'+str(fname)+'.csv')\n",
    "\n",
    "def defineVocabulary(data):\n",
    "    tempStorage = []\n",
    "    with nlp.select_pipes(disable=[\"parser\", \"lemmatizer\",\"spacytextblob\",]):\n",
    "        doc= nlp(data)\n",
    "        for token in doc:\n",
    "            tempStorage.append({token.text,token.pos_})\n",
    "    return tempStorage\n",
    "\n",
    "def removeStopWords(data):    \n",
    "    tempStorage = ast.literal_eval(data)    \n",
    "    # Load stop words\n",
    "    spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "    # Loop inside text\n",
    "    for x in tempStorage:\n",
    "        if x in spacy_stopwords: tempStorage.remove(x) \n",
    "    return tempStorage\n",
    "\n",
    "def definePolarization(data):   \n",
    "    with nlp.select_pipes(disable=[\"tok2vec\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "        doc= nlp(data)\n",
    "    return doc._.polarity\n",
    "\n",
    "def defineSubjectivity(data):\n",
    "    with nlp.select_pipes(disable=[\"tok2vec\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "        doc= nlp(data)\n",
    "    return doc._.subjectivity\n",
    "\n",
    "def defineAssessments(data):\n",
    "    with nlp.select_pipes(disable=[\"tok2vec\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "        doc= nlp(data)\n",
    "    return doc._.assessments\n",
    "    \n",
    "def emoji_to_word(text):\n",
    "    return emoji.demojize(text, delimiters=(\"\", \"\"))\n",
    "\n",
    "# Tokenize the text in the dataframe\n",
    "def tokenizeText(data):\n",
    "    tempStorage = []\n",
    "    with nlp.select_pipes(disable=[\"tagger\", \"parser\", \"lemmatizer\"]):\n",
    "        doc = nlp(data)\n",
    "        for word in doc:\n",
    "            tempStorage.append(word.text)\n",
    "    return tempStorage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generateUniqueWords(data):\n",
    "\n",
    "    no_top_unique_words = \"50\" #@param {type:\"string\"}\n",
    "\n",
    "    result = Counter(\" \".join(tweet_df['text'].values.tolist()).split(\" \")).items()\n",
    "    df2 = pd.DataFrame(result)\n",
    "    df2.columns =['Word', 'Frequency']\n",
    "    df2 = df2[df2.Word != \"\"] #Deletes the empty spaces counted\n",
    "    df2 = df2.sort_values(['Frequency'], ascending=[False]) #Sort dataframe by frequency (Descending)\n",
    "\n",
    "    print('\\033[1mTop '+no_top_unique_words+' most unique words used from the dataset\\033[0m \\n')\n",
    "    print(df2.head(int(no_top_unique_words)).to_string(index=False)) #Prints the top N unique words used\n",
    "    print(\"\\n\")\n",
    "    df3 = df2.head(int(no_top_unique_words))\n",
    "    df3.plot(y='Frequency', kind='pie', labels=df3['Word'], figsize=(9, 9), autopct='%1.1f%%', title='Top '+no_top_unique_words+' most unique words used from the dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWordcloudData(payload):    \n",
    "    \"\"\"\n",
    "      This Function Accepts a pandas dataframe from any step of the processing with the named text column.\n",
    "      payload - pandas dataframe with string text column.\n",
    "    \"\"\"   \n",
    "    \n",
    "    combinedText = \"\"\n",
    "    \n",
    "    textData = payload['text']\n",
    "    textData = pd.DataFrame(textData)\n",
    "    \n",
    "    combinedText = combinedText.join(textData.text)\n",
    "\n",
    "    #image storage\n",
    "    # https://drive.google.com/file/d/14DRvgevx-ffa9BKPztzt3nE9B2TamWhJ/view?usp=sharing\n",
    "    mask = np.array(Image.open(\"./img/cov19.png\")) #jpg also ok\n",
    "    stopwords = set(STOPWORDS)\n",
    "    wordcloud = WordCloud(\n",
    "                            max_font_size = 140,\n",
    "                            width = 2000,\n",
    "                            height = 1000,\n",
    "                            background_color = \"white\",\n",
    "                            mask=mask,\n",
    "                            contour_width = 1,\n",
    "                            stopwords = STOPWORDS)\n",
    "\n",
    "    wordcloud.generate(combinedText)\n",
    "\n",
    "    plt.imshow(wordcloud)#, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.figure()\n",
    "#     plt.imshow(mask)#, cmap=plt.cm.gray, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "#     plt.show()\n",
    "    #write to img\n",
    "    image = wordcloud.to_image()\n",
    "    image.show()    \n",
    "    \n",
    "    return plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create A Sample Dataset to process\n",
    "createDataSample(datafile =  './complete/20200730.json', sample_count = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0\n",
    "# Specify Raw Tweet Directory\n",
    "tweetDir = './tweets'\n",
    "_, _, filenames = next(os.walk(tweetDir))\n",
    "\n",
    "for file in filenames:\n",
    "    extractTweetText(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1\n",
    "#Extract Text of Tweets and Save to CSV file\n",
    "\n",
    "tweetDirS1 = './step_1'\n",
    "_, _, filenames_s1 = next(os.walk(tweetDirS1))\n",
    "\n",
    "for file in filenames_s1:\n",
    "    start = timer()\n",
    "    removeTwitterMeta(file)\n",
    "    end = timer()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# Get The Files inside the step_2 folder for futher processing\n",
    "tweetDirS2 = './step_2'\n",
    "_, _, filenames_s2 = next(os.walk(tweetDirS2))\n",
    "\n",
    "for file in filenames_s2:\n",
    "    start = timer()    \n",
    "    mydata = pd.read_csv('./step_2/'+file,index_col=False)\n",
    "    del mydata['Unnamed: 0'] \n",
    "    mydata['ttext'] = mydata['text'].progress_apply(lambda x : tokenizeText(x))\n",
    "    \n",
    "    #Get only the file name and remove the extension\n",
    "    fname = str(file).split('_')[1]\n",
    "    fname = str(fname).split('.')[0]\n",
    "    mydata.to_csv(r'./step_3/s3_'+str(fname)+'.csv')\n",
    "    end = timer()\n",
    "    print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "# Get The Files inside the step_3 folder and reduce the connecting words\n",
    "tweetDirS3 = './step_3'\n",
    "_, _, filenames_s3 = next(os.walk(tweetDirS3))\n",
    "\n",
    "for file in filenames_s3:\n",
    "    start = timer()\n",
    "    mydata = pd.read_csv('./step_3/'+file,index_col=False)\n",
    "    del mydata['Unnamed: 0']\n",
    "    mydata['nstop'] = mydata['ttext'].progress_apply(lambda x : removeStopWords(x))\n",
    "    \n",
    "    #Get only the file name and remove the extension\n",
    "    fname = str(file).split('_')[1]\n",
    "    fname = str(fname).split('.')[0]\n",
    "    mydata.to_csv(r'./step_4/s4_'+str(fname)+'.csv')\n",
    "    end = timer()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4\n",
    "# Get The Files inside the step_5 folder add column for polarity, assessment and subjectivity\n",
    "\n",
    "tweetDirS4 = './step_4'\n",
    "_, _, filenames_s4 = next(os.walk(tweetDirS4))\n",
    "\n",
    "for file in filenames_s4:\n",
    "    start = timer()\n",
    "    mydata = pd.read_csv('./step_4/'+file,index_col=False)\n",
    "    del mydata['Unnamed: 0'] \n",
    "    mydata['polarization'] = mydata['text'].progress_apply(lambda x : definePolarization(x))\n",
    "    mydata['subjectivity'] = mydata['text'].progress_apply(lambda x : defineSubjectivity(x))\n",
    "    mydata['assessment'] = mydata['text'].progress_apply(lambda x : defineAssessments(x))\n",
    "    \n",
    "    #Get only the file name and remove the extension\n",
    "    fname = str(file).split('_')[1]\n",
    "    fname = str(fname).split('.')[0]\n",
    "    mydata.to_csv(r'./step_5/s5_'+str(fname)+'.csv')\n",
    "    end = timer()\n",
    "    print(end - start)\n",
    "# Seperate wordscore in 3 seperate columns\n",
    "mydata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateWordcloudData(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 OPTIONAL\n",
    "# Get The Files inside the step_4 folder check\n",
    "\n",
    "tweetDirS5 = './step_5'\n",
    "_, _, filenames_s5 = next(os.walk(tweetDirS5))\n",
    "\n",
    "for file in filenames_s5:\n",
    "    start = timer()\n",
    "    mydata = pd.read_csv('./step_5/'+file,index_col=False)\n",
    "    del mydata['Unnamed: 0'] \n",
    "    processingFrame = mydata['text'].copy()\n",
    "    processingFrame = pd.DataFrame(processingFrame)\n",
    "    processingFrame['wordDef'] = processingFrame['text'].progress_apply(lambda x : defineVocabulary(x))\n",
    "    mydata['wordDef'] = processingFrame['wordDef'].copy()\n",
    "    #Get only the file name and remove the extension\n",
    "    fname = str(file).split('_')[1]\n",
    "    fname = str(fname).split('.')[0]\n",
    "    mydata.to_csv(r'./step_6/s6_'+str(fname)+'.csv')\n",
    "    end = timer()\n",
    "    print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Varya\n",
    "# Update wordcloud\n",
    "# install wordcloud\n",
    "# current problem: reads whole text at once \n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    max_font_size=140,\n",
    "    width=2000,\n",
    "    height=1000,\n",
    "    background_color=\"white\",\n",
    "    mask=mask,\n",
    "    contour_width=1,\n",
    "    stopwords=STOPWORDS)\n",
    "\n",
    "\n",
    "mask = np.array(Image.open(\"cov19.png\")) #jpg also ok\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "with open('preprocessed_feb.txt') as file:\n",
    "    text = file.read() # :(\n",
    "    \n",
    "wordcloud.generate(text)\n",
    "    \n",
    "plt.imshow(wordcloud)#, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.figure()\n",
    "plt.imshow(mask)#, cmap=plt.cm.gray, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "#write to img\n",
    "image = wordcloud.to_image()\n",
    "image.show()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Varya\n",
    "# convert emoji to names\n",
    "import unicodedata\n",
    "def emoji_to_name(text):\n",
    "    for symbol in text:    \n",
    "        if unicodedata.category(symbol) == 'So':\n",
    "            name = unicodedata.name(symbol) #returns all names uppercase\n",
    "            print(name)\n",
    "            text = emo.replace(symbol, ' '+name)\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
